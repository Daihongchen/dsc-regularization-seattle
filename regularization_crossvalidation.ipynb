{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "One of the goals of a machine learning project is to make models which are highly predictive.\n",
    "If the model fails to generalize to unseen data then the model is bad\n",
    "\n",
    "\n",
    "## Review of Earlier concepts\n",
    "\n",
    "### Overfitting vs Underfitting\n",
    "    1. Underfit Models fail to capture all of the information in the data\n",
    "        1. Ex. the mean leaves a lot of information on the table\n",
    "    2. Overfit models fit to the noise in the data and fail to generalize\n",
    "    3. How would we know if our model is over or underfit?\n",
    "        1. Train test split\n",
    "        2. Look at the testing error\n",
    "        3. As model complexity increases so does the possibility for overfitting\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias variance tradeoff\n",
    "    1. High bias\n",
    "        1. Systematic error in predictions (ie the average)\n",
    "        2. Bias is about the strength of assumptions the model makes\n",
    "        3. Underfit models tend to be high bias\n",
    "    2. High variance\n",
    "        1. The model is highly sensitive to changes in the data\n",
    "        2. Overfit models tend to be low bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization - a way to prevent over fitting\n",
    "    1. Types of regularization\n",
    "        1. Reducing the amount of features\n",
    "        2. Increase the amount of data\n",
    "        3. Ridge, lasso, elastic net\n",
    "        \n",
    "Again, complex models are very flexible in the patterns that they can model but this also means that they can easily find patterns that are simply statistical flukes of one particular dataset rather than patterns reflective of the underlying data generating process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge and Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads = pd.read_csv('Advertising.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ads[['TV',\n",
    "                                                         'radio',\n",
    "                                                         'newspaper']],\n",
    "                                                   ads['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = LinearRegression()\n",
    "lr1.fit(X_train, y_train)\n",
    "\n",
    "ads_preds = lr1.predict(X_test)\n",
    "\n",
    "np.sqrt(mean_squared_error(ads_preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(ads);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try and create your own polynomial features with degree 2 in sklearn and create a linear model\n",
    "# Answer is at the bottom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "pf = PolynomialFeatures()\n",
    "\n",
    "X_train_processed = pf.fit_transform(ss.fit_transform(X_train))\n",
    "X_test_processed = pf.transform(ss.transform(X_test))\n",
    "\n",
    "#alpha == lambda since lamda is a key word in python\n",
    "rr = Ridge(alpha=1)\n",
    "\n",
    "rr.fit(X_train_processed, y_train)\n",
    "ridge_preds = rr.predict(X_test_processed)\n",
    "\n",
    "np.sqrt(mean_squared_error(ridge_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why did I pick an alpha of 1? Is there a principled way to choose this hyperparamter?\n",
    "\n",
    "Create many models on the training set and see which one does best on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation\n",
    "\n",
    "How can we know what values of lamda to choose for ridge or lasso regression? Crossvalidation!\n",
    "\n",
    "We have already discussed the train test split and that is the basis for all other kinds of crossvalidation. We can extend this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to find the optimal value for alpha/lamda for lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures()\n",
    "X_poly_train = pf.fit_transform(X_train[['TV', 'radio', 'newspaper']])\n",
    "\n",
    "pf_sales = pd.DataFrame(X_poly, columns=pf.get_feature_names(['TV', 'radio', 'newspaper']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_y = pd.concat([pf_sales[['TV', 'radio', 'newspaper', 'TV radio',\n",
    "                            'TV newspaper']],\n",
    "                  ads['sales']],  axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_lr = LinearRegression()\n",
    "poly_lr.fit(X_poly_train, y_train)\n",
    "\n",
    "\n",
    "X_poly_test = pf.transform(X_test[['TV', 'radio', 'newspaper']])\n",
    "poly_preds = poly_lr.predict(X_poly_test)\n",
    "\n",
    "np.sqrt(mean_squared_error(poly_preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(pf_sales.columns, poly_lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pf_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
